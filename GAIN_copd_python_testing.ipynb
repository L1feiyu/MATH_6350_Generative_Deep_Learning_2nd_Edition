{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失值比例（添加后）:\n",
      "output_1    0.2\n",
      "output_2    0.2\n",
      "output_3    0.2\n",
      "output_4    0.2\n",
      "output_5    0.2\n",
      "output_6    0.2\n",
      "dtype: float64\n",
      "[100/2000] D=0.7505 | G=0.1447 | MSE=0.0211\n",
      "[200/2000] D=0.7419 | G=0.1228 | MSE=0.0225\n",
      "[300/2000] D=0.7223 | G=0.1327 | MSE=0.0242\n",
      "[400/2000] D=0.6995 | G=0.1229 | MSE=0.0217\n",
      "[500/2000] D=0.6818 | G=0.1377 | MSE=0.0209\n",
      "[600/2000] D=0.6623 | G=0.1138 | MSE=0.0217\n",
      "[700/2000] D=0.6563 | G=0.1371 | MSE=0.0212\n",
      "[800/2000] D=0.6430 | G=0.1347 | MSE=0.0241\n",
      "[900/2000] D=0.6384 | G=0.1216 | MSE=0.0238\n",
      "[1000/2000] D=0.6254 | G=0.1235 | MSE=0.0216\n",
      "[1100/2000] D=0.6026 | G=0.1376 | MSE=0.0202\n",
      "[1200/2000] D=0.6003 | G=0.1137 | MSE=0.0205\n",
      "[1300/2000] D=0.5724 | G=0.1130 | MSE=0.0251\n",
      "[1400/2000] D=0.5571 | G=0.1196 | MSE=0.0236\n",
      "[1500/2000] D=0.5457 | G=0.1259 | MSE=0.0211\n",
      "[1600/2000] D=0.5436 | G=0.1072 | MSE=0.0232\n",
      "[1700/2000] D=0.5318 | G=0.1190 | MSE=0.0212\n",
      "[1800/2000] D=0.5045 | G=0.0978 | MSE=0.0207\n",
      "[1900/2000] D=0.4972 | G=0.0995 | MSE=0.0210\n",
      "[2000/2000] D=0.4830 | G=0.0945 | MSE=0.0221\n",
      "✅ Imputation completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "\n",
    "# Step 1 : add mask\n",
    "\n",
    "# 添加随机缺失到指定列\n",
    "np.random.seed(42)  # 可复现性\n",
    "gain_cols = ['output_1', 'output_2', 'output_3', 'output_4', 'output_5', 'output_6']\n",
    "missing_rate = 0.2  # 缺失率 20%\n",
    "\n",
    "df = pd.read_csv(\"simulated_23_anomaly_dataset.csv\")\n",
    "\n",
    "# 创建缺失 mask，并将对应位置设为 np.nan\n",
    "for col in gain_cols:\n",
    "    mask = np.random.rand(len(df)) < missing_rate\n",
    "    df.loc[mask, col] = np.nan\n",
    "\n",
    "# 查看每列缺失比例\n",
    "print(\"缺失值比例（添加后）:\")\n",
    "print(df[gain_cols].isnull().mean().round(2))\n",
    "\n",
    "\n",
    "gain_cols = ['output_1', 'output_2', 'output_3',\n",
    "             'output_4', 'output_5', 'output_6']\n",
    "data_x = df[gain_cols].values.astype(np.float32)\n",
    "\n",
    "# Step 2: Normalization helpers\n",
    "def normalization(data_x):\n",
    "    mins = np.nanmin(data_x, axis=0)\n",
    "    maxs = np.nanmax(data_x, axis=0)\n",
    "    ranges = np.where(maxs - mins == 0, 1.0, maxs - mins)\n",
    "    norm = (data_x - mins) / ranges\n",
    "    norm[np.isnan(data_x)] = np.nan\n",
    "    return norm, {'min': mins, 'max': maxs}\n",
    "\n",
    "def renormalization(norm_x, params):\n",
    "    return norm_x * (params['max'] - params['min']) + params['min']\n",
    "\n",
    "# Step 3: GAIN model\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(dim, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(dim, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(dim, activation='sigmoid')\n",
    "    def call(self, X, M, training=False):\n",
    "        h = tf.concat([X, M], axis=1)\n",
    "        return self.out(self.d2(self.d1(h)))\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(dim, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(dim, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(dim, activation='sigmoid')\n",
    "    def call(self, X, H, training=False):\n",
    "        h = tf.concat([X, H], axis=1)\n",
    "        return self.out(self.d2(self.d1(h)))\n",
    "\n",
    "# Step 4: GAIN training and imputation\n",
    "def gain_tf2(data_x, params):\n",
    "    norm_x, norm_params = normalization(data_x)\n",
    "    miss_mask = (~np.isnan(norm_x)).astype(np.float32)\n",
    "    norm_x_filled = np.nan_to_num(norm_x)\n",
    "\n",
    "    n, dim = norm_x.shape\n",
    "    bs, hint_rate, alpha, iters = params.values()\n",
    "\n",
    "    G, D = Generator(dim), Discriminator(dim)\n",
    "    G_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "    D_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "    for it in range(1, iters + 1):\n",
    "        idx = np.random.choice(n, bs, replace=False)\n",
    "        Xb = norm_x_filled[idx]\n",
    "        Mb = miss_mask[idx]\n",
    "        Zb = np.random.uniform(0, 0.01, size=Xb.shape)\n",
    "        Hb = (np.random.uniform(0, 1, size=Xb.shape) < hint_rate) * Mb\n",
    "\n",
    "        Xb_input = Mb * Xb + (1 - Mb) * Zb\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            X_tensor = tf.convert_to_tensor(Xb_input, dtype=tf.float32)\n",
    "            M_tensor = tf.convert_to_tensor(Mb, dtype=tf.float32)\n",
    "            H_tensor = tf.convert_to_tensor(Hb, dtype=tf.float32)\n",
    "\n",
    "            G_sample = G(X_tensor, M_tensor, training=True)\n",
    "            Hat_X = M_tensor * X_tensor + (1 - M_tensor) * G_sample\n",
    "            D_prob = D(Hat_X, H_tensor, training=True)\n",
    "\n",
    "            D_loss = -tf.reduce_mean(M_tensor * tf.math.log(D_prob + 1e-8) +\n",
    "                                     (1 - M_tensor) * tf.math.log(1 - D_prob + 1e-8))\n",
    "            G_loss = -tf.reduce_mean((1 - M_tensor) * tf.math.log(D_prob + 1e-8))\n",
    "            MSE_loss = tf.reduce_mean((M_tensor * X_tensor - M_tensor * G_sample) ** 2) / tf.reduce_mean(M_tensor)\n",
    "            G_total = G_loss + alpha * MSE_loss\n",
    "\n",
    "        D_grads = tape.gradient(D_loss, D.trainable_variables)\n",
    "        G_grads = tape.gradient(G_total, G.trainable_variables)\n",
    "\n",
    "        D_opt.apply_gradients(zip(D_grads, D.trainable_variables))\n",
    "        G_opt.apply_gradients(zip(G_grads, G.trainable_variables))\n",
    "        del tape\n",
    "\n",
    "        if it % 100 == 0:\n",
    "            print(f\"[{it}/{iters}] D={D_loss:.4f} | G={G_loss:.4f} | MSE={MSE_loss:.4f}\")\n",
    "\n",
    "    # Final imputation\n",
    "    Z_full = np.random.uniform(0, 0.01, size=norm_x.shape)\n",
    "    X_input = miss_mask * norm_x_filled + (1 - miss_mask) * Z_full\n",
    "    imputed_norm = G(tf.convert_to_tensor(X_input, dtype=tf.float32), tf.convert_to_tensor(miss_mask)).numpy()\n",
    "    imputed = miss_mask * norm_x_filled + (1 - miss_mask) * imputed_norm\n",
    "    return renormalization(imputed, norm_params)\n",
    "\n",
    "# Step 5: Run GAIN\n",
    "params = {'batch_size': 128, 'hint_rate': 0.9, 'alpha': 100, 'iterations': 2000}\n",
    "imputed_output = gain_tf2(data_x, params)\n",
    "\n",
    "# Replace missing values in df\n",
    "df[gain_cols] = imputed_output\n",
    "df.to_csv(\"simulated_23_anomaly_dataset_imputed.csv\", index=False)\n",
    "print(\"✅ Imputation completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your data\n",
    "df = pd.read_csv(\"simulated_23_anomaly_dataset.csv\")\n",
    "\n",
    "\n",
    "gain_cols = ['output_1', 'output_2', 'output_3',\n",
    "             'output_4', 'output_5', 'output_6']\n",
    "data_x = df[gain_cols].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
